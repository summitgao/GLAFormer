{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNaZ2PNyTmfU"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tqj2NROBWJc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yhro3oEBm0q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage import io, measure\n",
        "import random\n",
        "import scipy.io as sio\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from collections import  Counter\n",
        "from __future__ import print_function\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import math\n",
        "import numbers\n",
        "from torch.nn import init\n",
        "from einops import rearrange\n",
        "from functools import partial\n",
        "from ipdb import set_trace as st\n",
        "\n",
        "torch.backends.cudnn.enable =True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFMlY0ceBvoU"
      },
      "outputs": [],
      "source": [
        "def acc_score(pre, lab):\n",
        "  num = 0\n",
        "  for i in range(pre.shape[0]):\n",
        "    if(pre[i] != lab[i]):\n",
        "      num+=1\n",
        "  return (pre.shape[0] - num) / pre.shape[0]\n",
        "\n",
        "#  Inputs:  gtImg  = ground truth image\n",
        "#           tstImg = change map\n",
        "#  Outputs: FA  = False alarms\n",
        "#           MA  = Missed alarms\n",
        "#           OE  = Overall error\n",
        "#           PCC = Overall accuracy\n",
        "def evaluate(gtImg, tstImg):\n",
        "  ylen = gtImg.shape[0]\n",
        "  FA = 0\n",
        "  MA = 0\n",
        "  label_0 = np.sum(gtImg==0)\n",
        "  label_1 = np.sum(gtImg==1)\n",
        "  print(\"label_0:\", label_0)\n",
        "  print(\"label_1:\", label_1)\n",
        "\n",
        "  for j in range(ylen):\n",
        "    if gtImg[j]==0 and tstImg[j]!=0 :\n",
        "        FA = FA+1\n",
        "    if gtImg[j]!=0 and tstImg[j]==0 :\n",
        "        MA = MA+1\n",
        "\n",
        "  OE = FA+MA\n",
        "  PCC = 1-OE/(ylen)\n",
        "  PRE=((label_1+FA-MA)*label_1+(label_0+MA-FA)*label_0)/((ylen)*(ylen))\n",
        "  KC=(PCC-PRE)/(1-PRE)\n",
        "  print(' Change detection results ==>')\n",
        "  print(' ... ... FP:  ', FA)\n",
        "  print(' ... ... FN:  ', MA)\n",
        "  print(' ... ... OE:  ', OE)\n",
        "  print(' ... ... PCC: ', format(PCC*100, '.2f'))\n",
        "  print(' ... ... KC: ', format(KC*100, '.2f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhxFu3Epsuk-"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "windowSize = 6\n",
        "epochs = 60\n",
        "# lr = 3e-5\n",
        "lr = 3e-5\n",
        "gamma = 0.7\n",
        "seed = 11\n",
        "batch_size = 32\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "seed_everything(seed)\n",
        "device = 'cuda'\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwEhNdtpmHle"
      },
      "outputs": [],
      "source": [
        "# # 获取index保存为.mat  格式为(x,y)\n",
        "# # im_gt = sio.loadmat('/content/drive/MyDrive/wzy/datasets/GTSa.mat')['GT']\n",
        "# im_gt = sio.loadmat('/content/drive/MyDrive/wzy/datasets/GTFarm.mat')['label']\n",
        "# im_gt = sio.loadmat('/content/drive/MyDrive/wzy/datasets/GTRiver.mat')['gt']\n",
        "\n",
        "# def getIndexMat(im_gt):\n",
        "\n",
        "#   ele_num1 = np.sum(im_gt==1)\n",
        "#   ele_num2 = np.sum(im_gt==2)\n",
        "\n",
        "#   index_1 = []\n",
        "#   index_2 = []\n",
        "#   index_all = []\n",
        "#   index_num1 = 0\n",
        "#   index_num2 = 0\n",
        "#   for i in range(im_gt.shape[0]):\n",
        "#     for j in range(im_gt.shape[1]):\n",
        "#       if(im_gt[i][j] == 1):\n",
        "#         index_1.append([i, j])\n",
        "#         index_num1 += 1\n",
        "#       elif im_gt[i][j] == 2:\n",
        "#         index_2.append([i, j])\n",
        "#         index_num2 += 1\n",
        "#       index_all.append([i,j])\n",
        "#   mat_path1 = '/content/drive/MyDrive/wzy/datasets/River_index_1.mat'\n",
        "#   mat_path2 = '/content/drive/MyDrive/wzy/datasets/River_index_2.mat'\n",
        "#   mat_path3 = '/content/drive/MyDrive/wzy/datasets/River_index_all.mat'\n",
        "#   io.savemat(mat_path1, {'index_1': index_1})\n",
        "#   io.savemat(mat_path2, {'index_2': index_2})\n",
        "#   io.savemat(mat_path3, {'index_all': index_all})\n",
        "# getIndexMat(im_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yumsUxTNg5oo"
      },
      "outputs": [],
      "source": [
        "\"\"\" Training dataset\"\"\"\n",
        "from torchvision.transforms import ToTensor\n",
        "class TrainTestDS(torch.utils.data.Dataset):\n",
        "  def __init__(self, hsi_1, hsi_2, gt, pos, windowSize):\n",
        "    self.pad = windowSize // 2\n",
        "    self.windowSize = windowSize\n",
        "    self.pos = pos\n",
        "    self.im1 = np.pad(hsi_1, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)), 'constant',constant_values=0)\n",
        "    self.im2 = np.pad(hsi_2, ((self.pad, self.pad), (self.pad, self.pad), (0, 0)), 'constant',constant_values=0)\n",
        "    self.gt = gt\n",
        "  def __getitem__(self, index):\n",
        "    # 根据索引返回数据和对应的标签\n",
        "    h, w = self.pos[index, :]\n",
        "    im1 = self.im1[h: h + self.windowSize, w: w + self.windowSize]\n",
        "    im2 = self.im2[h: h + self.windowSize, w: w + self.windowSize]\n",
        "    im1 = ToTensor()(im1).float()\n",
        "    im2 = ToTensor()(im2).float()\n",
        "    gt = torch.tensor(self.gt[h, w] - 1).long()\n",
        "\n",
        "    return im1, im2, gt\n",
        "  def __len__(self):\n",
        "    # 返回文件数据的数目\n",
        "    return self.pos.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6tGyCS4y-G0"
      },
      "outputs": [],
      "source": [
        "def getData(dataName, hsi_path, windowSize, keys):\n",
        "  hsi_path1 = hsi_path + '/' + dataName + '1.mat'\n",
        "  hsi_path2 = hsi_path + '/' + dataName + '2.mat'\n",
        "  gt_path = hsi_path + '/GT' + dataName + '.mat'\n",
        "  index_path1 = hsi_path + dataName +'_index_1.mat'\n",
        "  index_path2 = hsi_path + dataName +'_index_2.mat'\n",
        "  index_path3 = hsi_path + dataName +'_index_all.mat'\n",
        "  im1 = sio.loadmat(hsi_path1)[keys[0]]\n",
        "  im2 = sio.loadmat(hsi_path2)[keys[1]]\n",
        "  im_gt = sio.loadmat(gt_path)[keys[2]]\n",
        "  im_index_1 = sio.loadmat(index_path1)['index_1']\n",
        "  im_index_2 = sio.loadmat(index_path2)['index_2']\n",
        "  im_index_all = sio.loadmat(index_path3)['index_all']\n",
        "  h = im_gt.shape[0]\n",
        "  w = im_gt.shape[1]\n",
        "  c = im1.shape[2]\n",
        "\n",
        "  test_len = im_gt.shape[0] * im_gt.shape[1]   #设置测试index样本数\n",
        "  train_len=9000  #设置训练集index样本数\n",
        "\n",
        "  pdata=np.zeros((train_len, 2))\n",
        "  tdata=np.zeros((test_len, 2))\n",
        "\n",
        "  tdata[:,:] = im_index_all[:,:]\n",
        "  print(test_len)\n",
        "  np.random.shuffle(im_index_1)\n",
        "  np.random.shuffle(im_index_2)\n",
        "\n",
        "  pdata[0:6000,:]=im_index_1[0:6000,:]\n",
        "  pdata[6000:,:]=im_index_2[0:3000,:]\n",
        "\n",
        "  np.random.shuffle(pdata)\n",
        "\n",
        "  pdata = pdata.astype(np.int64)\n",
        "  tdata = tdata.astype(np.int64)\n",
        "\n",
        "  trainset = TrainTestDS(im1, im2, im_gt, pdata, windowSize)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  testset = TrainTestDS(im1, im2, im_gt, tdata, windowSize)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  return train_loader, test_loader, h, w, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4NtqchwyRN7"
      },
      "outputs": [],
      "source": [
        "keysFarm = ['imgh', 'imghl', 'label']\n",
        "keysRiver = ['river_before', 'river_after', 'gt']\n",
        "keysSa = ['T1', 'T2', 'GT']\n",
        "\n",
        "hsi_path = '/content/drive/MyDrive/wzy/HSI_CD/model/datasets/'\n",
        "\n",
        "# dataKey = keysFarm\n",
        "# dataName = 'Farm'\n",
        "\n",
        "dataKey = keysSa\n",
        "dataName = 'SSa'\n",
        "\n",
        "# dataKey = keysRiver\n",
        "# dataName = 'River'\n",
        "\n",
        "train_loader, test_loader, H, W, C = getData(dataName, hsi_path, windowSize, dataKey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjsCI706__O7"
      },
      "outputs": [],
      "source": [
        "class PixelEmbedding(nn.Module):\n",
        "    def __init__(self, num_bands, embed_dim):\n",
        "        super(PixelEmbedding, self).__init__()\n",
        "        self.fc = nn.Linear(num_bands, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1,windowSize*windowSize,embed_dim))\n",
        "        nn.init.trunc_normal_(self.positional_encoding, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len(9*9), num_bands)\n",
        "        x = self.fc(x)\n",
        "        x += self.positional_encoding\n",
        "        return x # [B , 81 ,384]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBCalG33__Rk"
      },
      "outputs": [],
      "source": [
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75JpPFR0l6uY"
      },
      "outputs": [],
      "source": [
        "class GLA(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=3, alpha=0.5):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "        head_dim = int(dim/num_heads)\n",
        "        self.dim = dim\n",
        "\n",
        "        self.l_heads = int(num_heads * alpha)\n",
        "        self.l_dim = self.l_heads * head_dim\n",
        "\n",
        "        self.h_heads = num_heads - self.l_heads\n",
        "        self.h_dim = self.h_heads * head_dim\n",
        "\n",
        "        self.ws = window_size\n",
        "\n",
        "        if self.ws == 1:\n",
        "            self.h_heads = 0\n",
        "            self.h_dim = 0\n",
        "            self.l_heads = num_heads\n",
        "            self.l_dim = dim\n",
        "\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        if self.l_heads > 0:\n",
        "            if self.ws != 1:\n",
        "                self.sr = nn.AvgPool2d(kernel_size=window_size, stride=window_size)\n",
        "            self.l_q = nn.Linear(self.dim, self.l_dim, bias=qkv_bias)\n",
        "            self.l_kv = nn.Linear(self.dim, self.l_dim * 2, bias=qkv_bias)\n",
        "            self.l_proj = nn.Linear(self.l_dim, self.l_dim)\n",
        "\n",
        "        if self.h_heads > 0:\n",
        "            self.h_qkv = nn.Linear(self.dim, self.h_dim * 3, bias=qkv_bias)\n",
        "            self.h_proj = nn.Linear(self.h_dim, self.h_dim)\n",
        "\n",
        "    def loc(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        h_group, w_group = H // self.ws, W // self.ws\n",
        "\n",
        "        total_groups = h_group * w_group\n",
        "\n",
        "        x = x.reshape(B, h_group, self.ws, w_group, self.ws, C).transpose(2, 3)\n",
        "\n",
        "        qkv = self.h_qkv(x).reshape(B, total_groups, -1, 3, self.h_heads, self.h_dim // self.h_heads).permute(3, 0, 1, 4, 2, 5)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # B, hw, n_head, ws*ws, head_dim\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale  # B, hw, n_head, ws*ws, ws*ws\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = (attn @ v).transpose(2, 3).reshape(B, h_group, w_group, self.ws, self.ws, self.h_dim)\n",
        "        x = attn.transpose(2, 3).reshape(B, h_group * self.ws, w_group * self.ws, self.h_dim)\n",
        "\n",
        "        x = self.h_proj(x)\n",
        "        return x\n",
        "\n",
        "    def glo(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        q = self.l_q(x).reshape(B, H * W, self.l_heads, self.l_dim // self.l_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        if self.ws > 1:\n",
        "            x_ = x.permute(0, 3, 1, 2)\n",
        "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
        "            kv = self.l_kv(x_).reshape(B, -1, 2, self.l_heads, self.l_dim // self.l_heads).permute(2, 0, 3, 1, 4)\n",
        "        else:\n",
        "            kv = self.l_kv(x).reshape(B, -1, 2, self.l_heads, self.l_dim // self.l_heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, H, W, self.l_dim)\n",
        "        x = self.l_proj(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, H, W):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        x = x.reshape(B, H, W, C)\n",
        "\n",
        "        if self.h_heads == 0:\n",
        "            x = self.glo(x)\n",
        "            return x.reshape(B, N, C)\n",
        "\n",
        "        if self.l_heads == 0:\n",
        "            x = self.loc(x)\n",
        "            return x.reshape(B, N, C)\n",
        "\n",
        "        loc_out = self.loc(x)\n",
        "        glo_out = self.glo(x)\n",
        "\n",
        "        x = torch.cat((loc_out, glo_out), dim=-1)\n",
        "        x = x.reshape(B, N, C)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K3EIBGO4mzk"
      },
      "outputs": [],
      "source": [
        "# Cross Gated Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor=2.66, bias=True):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,_ , C = x.shape\n",
        "        # x = torch.permute(x.view(B,9,9,C),(0,3,1,2)) # [B,C,9,9]\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w',h=windowSize,w=windowSize)\n",
        "\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x2)*x1 + F.gelu(x1)*x2\n",
        "        x = self.project_out(x)\n",
        "\n",
        "        # x = torch.permute(x,(0,2,3,1)).view(B,81,C) # [B,9*9,C]\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yndxup51SYL"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 num_heads,\n",
        "                 mlp_ratio=2.66,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=None,\n",
        "                 drop_ratio=0.,\n",
        "                 attn_drop_ratio=0.,\n",
        "                 drop_path_ratio=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm):\n",
        "        super(Block, self).__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        # self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "        #                       attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
        "\n",
        "        self.gla = GLA(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                              attn_drop=attn_drop_ratio, proj_drop=drop_ratio, window_size = 3,alpha=0.5)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.ffn = FeedForward(dim,ffn_expansion_factor=mlp_ratio)\n",
        "        # mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        # self.ffn = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.gla(self.norm1(x),windowSize, windowSize))  # norm1-->attn-->drop_path\n",
        "        x = x + self.drop_path(self.ffn(self.norm2(x)))   # norm2-->MLP(FFN)-->drop_path\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcMMt8hdAk8A"
      },
      "outputs": [],
      "source": [
        "def _init_vit_weights(m):\n",
        "    \"\"\"\n",
        "    ViT weight initialization\n",
        "    :param m: module\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.trunc_normal_(m.weight, std=.01)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.zeros_(m.bias)\n",
        "        nn.init.ones_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZkXuzZj1Sat"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GLAFormer(nn.Module):\n",
        "    def __init__(self,  img_c, num_classes=2,\n",
        "                 embed_dim=128, depth=4, num_heads=8, mlp_ratio=4.0, qkv_bias=True,\n",
        "                 qk_scale=None, drop_ratio=0.,\n",
        "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PixelEmbedding, norm_layer=None,\n",
        "                 act_layer=None):\n",
        "\n",
        "        super(GLAFormer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.num_tokens = 1\n",
        "        img1_norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        img1_act_layer = act_layer or nn.GELU\n",
        "        img2_norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        img2_act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.img1_patch_embed = embed_layer(num_bands = img_c, embed_dim = embed_dim)\n",
        "        self.img2_patch_embed = embed_layer(num_bands = img_c, embed_dim = embed_dim)\n",
        "\n",
        "        img1_dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
        "        self.img1_blocks = nn.Sequential(*[\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=img1_dpr[i],\n",
        "                  norm_layer=img1_norm_layer, act_layer=img1_act_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.img1_norm = img1_norm_layer(embed_dim)\n",
        "\n",
        "        img2_dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
        "        self.img2_blocks = nn.Sequential(*[\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=img1_dpr[i],\n",
        "                  norm_layer=img2_norm_layer, act_layer=img2_act_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.img2_norm = img2_norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head(s)\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=embed_dim, out_channels=16, kernel_size=3, padding=1),nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, padding=1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(windowSize*windowSize, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 2)\n",
        "        )\n",
        "\n",
        "        self.apply(_init_vit_weights)\n",
        "\n",
        "    def forward_features(self, img):\n",
        "        x = self.patch_embed(x , y)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        img1 = self.img1_patch_embed(img1)\n",
        "        img1 = self.img1_blocks(img1)\n",
        "        img1 = self.img1_norm(img1)\n",
        "\n",
        "\n",
        "        img2 = self.img1_patch_embed(img2)\n",
        "        img2 = self.img1_blocks(img2)\n",
        "        img2 = self.img1_norm(img2)\n",
        "\n",
        "        fuse_feature = torch.abs(torch.sub(img1, img2))\n",
        "        # print(fuse_feature.shape)\n",
        "        B,_ , C = fuse_feature.shape\n",
        "        x = torch.torch.permute(fuse_feature.view(B,windowSize,windowSize,C),(0,3,1,2))\n",
        "        # print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(B,-1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1kC8Fl11Sc4"
      },
      "outputs": [],
      "source": [
        "model = GLAFormer(img_c=C,\n",
        "                embed_dim=256,\n",
        "                depth=6,\n",
        "                num_heads=8,\n",
        "                num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBcSkJB_omwN"
      },
      "outputs": [],
      "source": [
        "def save():\n",
        "  model.eval()\n",
        "  count = 0\n",
        "  max_acc = 0.\n",
        "  for im1_loader, im2_loader, label in tqdm(test_loader):\n",
        "    im1_loader = im1_loader.to(device)\n",
        "    im2_loader = im2_loader.to(device)\n",
        "    im1_loader = torch.transpose(torch.flatten(im1_loader, start_dim=2,end_dim=3),1,2)\n",
        "    im2_loader = torch.transpose(torch.flatten(im2_loader, start_dim=2,end_dim=3),1,2)\n",
        "    predict = model(im1_loader,im2_loader)\n",
        "    predict = np.argmax(predict.detach().cpu().numpy(), axis=1)\n",
        "\n",
        "    if count == 0:\n",
        "      y_pred_test =  predict\n",
        "      gty = label\n",
        "      count = 1\n",
        "    else:\n",
        "      y_pred_test = np.concatenate( (y_pred_test, predict) )\n",
        "      gty = np.concatenate( (gty, label) )\n",
        "  acc1 = acc_score(y_pred_test, gty)\n",
        "  if acc1 > max_acc:\n",
        "    torch.save(model, '/content/drive/MyDrive/wzy/HSI_CD/model/Sa/H_D/model_H_D_W6{:.4f}.pth'.format(acc1))\n",
        "    max_acc = acc1\n",
        "\n",
        "  evaluate(y_pred_test, gty)\n",
        "\n",
        "  outputs = np.zeros((H, W))\n",
        "  for i in range(H):\n",
        "    outputs[i,:] = y_pred_test[i*W:(i+1)*W]\n",
        "  res = outputs*255\n",
        "  res = res.astype(np.int64)\n",
        "  plt.imshow(res, 'gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02Q44oRIpWms"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0\n",
        "  epoch_accuracy = 0\n",
        "  for im1_loader, im2_loader, label in tqdm(train_loader):\n",
        "    im1_loader = im1_loader.to(device)\n",
        "    im2_loader = im2_loader.to(device)\n",
        "\n",
        "    # [P,81,C]\n",
        "    im1_loader = torch.transpose(torch.flatten(im1_loader, start_dim=2,end_dim=3),1,2)\n",
        "    im2_loader = torch.transpose(torch.flatten(im2_loader, start_dim=2,end_dim=3),1,2)\n",
        "    label = label.to(device)\n",
        "    output = model(im1_loader, im2_loader)\n",
        "\n",
        "    loss = criterion(output, label)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    acc = (output.argmax(dim=1) == label).float().mean()\n",
        "    epoch_accuracy += acc / len(train_loader)\n",
        "    epoch_loss += loss / len(train_loader)\n",
        "  print(\n",
        "      f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f}\\n\"\n",
        "  )\n",
        "  if (epoch+1)%20==0:\n",
        "    save()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
